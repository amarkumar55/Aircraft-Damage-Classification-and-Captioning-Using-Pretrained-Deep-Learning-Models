{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4bd98ec",
   "metadata": {},
   "source": [
    "### Aircraft Damage Classification and Captioning Using Pretrained Deep Learning Models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f30f74",
   "metadata": {},
   "source": [
    "Aircraft damage inspection is a critical task for ensuring flight safety and maintaining aircraft structural integrity. Traditional manual inspection methods are time-consuming, costly, and susceptible to human error. This project presents an automated deep learning approach to classify aircraft damage and generate descriptive captions for damaged aircraft images.\n",
    "\n",
    "The project focuses on two main tasks:\n",
    "\n",
    "1. Damage Classification – Classifying aircraft damage into two categories:\n",
    "   Dent and Crack, using feature extraction with a pre-trained VGG16 convolutional neural network.\n",
    "\n",
    "2. Damage Captioning and Summarization – Generating natural-language descriptions and summaries of aircraft damage images using a pre-trained Transformer-based model, enabling explainable and interpretable outputs.\n",
    "\n",
    "By combining computer vision and natural language processing techniques, this project demonstrates how deep learning can be applied to automate aircraft inspection workflows, reduce manual effort, and improve maintenance efficiency in the aviation industry.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "717d6221",
   "metadata": {},
   "source": [
    "### Import Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eef86c0",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import keras\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.applications import VGG16\n",
    "from keras.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from keras.preprocessing import image\n",
    "import random\n",
    "\n",
    "seed_value = 42\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "tf.random.set_seed(seed_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "090e8ad1",
   "metadata": {},
   "source": [
    "### Configuration for data loading\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe5d67a1",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "n_epochs = 10\n",
    "img_rows, img_cols = 224, 224\n",
    "input_shape = (img_rows, img_cols, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f9eb13",
   "metadata": {},
   "source": [
    "### Extract dataset and prepare\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67047784",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import tarfile\n",
    "import urllib.request\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "url = \"DATASET_URL\"\n",
    "\n",
    "# Define the path to save the file\n",
    "tar_filename = \"aircraft_damage_dataset_v1.tar\"\n",
    "extracted_folder = \"aircraft_damage_dataset_v1\"  # Folder where contents will be extracted\n",
    "\n",
    "# Download the tar file\n",
    "urllib.request.urlretrieve(url, tar_filename)\n",
    "print(f\"Downloaded {tar_filename}. Extraction will begin now.\")\n",
    "\n",
    "# Check if the folder already exists\n",
    "if os.path.exists(extracted_folder):\n",
    "    print(f\"The folder '{extracted_folder}' already exists. Removing the existing folder.\")\n",
    "    \n",
    "    # Remove the existing folder to avoid overwriting or duplication\n",
    "    shutil.rmtree(extracted_folder)\n",
    "    print(f\"Removed the existing folder: {extracted_folder}\")\n",
    "\n",
    "# Extract the contents of the tar file\n",
    "with tarfile.open(tar_filename, \"r\") as tar_ref:\n",
    "    tar_ref.extractall()  # This will extract to the current directory\n",
    "    print(f\"Extracted {tar_filename} successfully.\")\n",
    "\n",
    "\n",
    "extract_path = \"aircraft_damage_dataset_v1\"\n",
    "train_dir = os.path.join(extract_path, 'train')\n",
    "test_dir = os.path.join(extract_path, 'test')\n",
    "valid_dir = os.path.join(extract_path, 'valid')\n",
    "\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "valid_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(img_rows, img_cols),   # Resize images to the size VGG16 expects\n",
    "    batch_size=batch_size,\n",
    "    seed = seed_value,\n",
    "    class_mode='binary',\n",
    "    shuffle=True # Binary classification: dent vs crack\n",
    ")\n",
    "\n",
    "valid_generator = valid_datagen.flow_from_directory(\n",
    "    directory=valid_dir,\n",
    "    class_mode='binary',\n",
    "    seed=seed_value,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    target_size=(img_rows, img_cols)\n",
    ")\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    directory=test_dir,\n",
    "    class_mode='binary',\n",
    "    seed=seed_value,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    target_size=(img_rows, img_cols)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e1d074b",
   "metadata": {},
   "source": [
    "### Model defination using pretrain model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9baf49b0",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "base_model = VGG16(\n",
    "    weights=\"imagenet\",\n",
    "    include_top=False,\n",
    "    input_shape=(img_rows, img_cols, 3)\n",
    ")\n",
    "\n",
    "\n",
    "base_model.trainable = False\n",
    "\n",
    "model = Sequential([\n",
    "    base_model,\n",
    "    Flatten(),\n",
    "    Dense(512, activation=\"relu\"),\n",
    "    Dropout(0.3),\n",
    "    Dense(512, activation=\"relu\"),\n",
    "    Dropout(0.3),\n",
    "    Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "\n",
    "\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=0.0001),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540e8b5e",
   "metadata": {},
   "source": [
    "### Model Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87979ac2",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    train_generator,\n",
    "    epochs=n_epochs,\n",
    "    validation_data=valid_generator\n",
    ")\n",
    "\n",
    "train_history = model.history.history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb85608d",
   "metadata": {},
   "source": [
    "### Model Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e42fbd1",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "test_loss, test_accuracy = model.evaluate(test_generator, steps=test_generator.samples // test_generator.batch_size)\n",
    "\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f79f09",
   "metadata": {},
   "source": [
    "### Display Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c25456df",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.preprocessing import image\n",
    "\n",
    "# Function to plot a single image and its prediction\n",
    "def plot_image_with_title(image, model, true_label, predicted_label, class_names):\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.imshow(image)\n",
    "\n",
    "    # Convert labels from one-hot to class indices if needed, but for binary labels it's just 0 or 1\n",
    "    true_label_name = class_names[true_label]  # Labels are already in class indices\n",
    "    pred_label_name = class_names[predicted_label]  # Predictions are 0 or 1\n",
    "\n",
    "    plt.title(f\"True: {true_label_name}\\nPred: {pred_label_name}\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Function to test the model with images from the test set\n",
    "def test_model_on_image(test_generator, model, index_to_plot=0):\n",
    "    # Get a batch of images and labels from the test generator\n",
    "    test_generator.reset()\n",
    "    test_images, test_labels = next(test_generator)\n",
    "\n",
    "    # Make predictions on the batch\n",
    "    predictions = model.predict(test_images)\n",
    "\n",
    "    # In binary classification, predictions are probabilities (float). Convert to binary (0 or 1)\n",
    "    predicted_classes = (predictions > 0.5).astype(int).flatten()\n",
    "\n",
    "    # Get the class indices from the test generator and invert them to get class names\n",
    "    class_indices = test_generator.class_indices\n",
    "    class_names = {v: k for k, v in class_indices.items()}  # Invert the dictionary\n",
    "\n",
    "    # Specify the image to display based on the index\n",
    "    image_to_plot = test_images[index_to_plot]\n",
    "    true_label = test_labels[index_to_plot]\n",
    "    predicted_label = predicted_classes[index_to_plot]\n",
    "\n",
    "    # Plot the selected image with its true and predicted labels\n",
    "    plot_image_with_title(image=image_to_plot, model=model, true_label=true_label, predicted_label=predicted_label, class_names=class_names)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4975a3e2",
   "metadata": {},
   "source": [
    "### Image captioning and summarization using BLIP model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4aa1b8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# NOTE:\n",
    "# BLIP is a PyTorch-based Transformer model.\n",
    "# TensorFlow's tf.py_function is used here to integrate\n",
    "# PyTorch inference into a TensorFlow workflow for demonstration purposes.\n",
    "\n",
    "\n",
    "import torch\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "\n",
    "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "blip_model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "\n",
    "\n",
    "\n",
    "class BlipCaptionSummaryLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, processor, blip_model, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.processor = processor\n",
    "        self.model = blip_model\n",
    "\n",
    "\n",
    "    def call(self, image_path, task):\n",
    "        # Use tf.py_function to run the custom image processing and text generation\n",
    "        return tf.py_function(self.process_image, [image_path, task], tf.string)\n",
    "\n",
    "    def process_image(self, image_path, task):\n",
    "        \"\"\"\n",
    "        Perform image loading, preprocessing, and text generation.\n",
    "\n",
    "        Args:\n",
    "            image_path: Path to the image file as a string.\n",
    "            task: The type of task (\"caption\" or \"summary\").\n",
    "\n",
    "        Returns:\n",
    "            The generated caption or summary as a string.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Decode the image path from the TensorFlow tensor to a Python string\n",
    "            image_path_str = image_path.numpy().decode(\"utf-8\")\n",
    "\n",
    "            # Open the image using PIL and convert it to RGB format\n",
    "            image = Image.open(image_path_str).convert(\"RGB\")\n",
    "\n",
    "            # Set the appropriate prompt based on the task\n",
    "            if task.numpy().decode(\"utf-8\") == \"caption\":\n",
    "                prompt = \"This is a picture of\"  # Modify prompt for more natural output\n",
    "            else:\n",
    "                prompt = \"This is a detailed photo showing\"  # Modify for summary\n",
    "\n",
    "            # Prepare inputs for the BLIP model\n",
    "            inputs = self.processor(images=image, text=prompt, return_tensors=\"pt\")\n",
    "\n",
    "            # Generate text output using the BLIP model\n",
    "            output = self.model.generate(**inputs)\n",
    "\n",
    "            # Decode the output into a readable string\n",
    "            result = self.processor.decode(output[0], skip_special_tokens=True)\n",
    "            return result\n",
    "        except Exception as e:\n",
    "            # Handle errors during image processing or text generation\n",
    "            print(f\"Error: {e}\")\n",
    "            return \"Error processing image\"\n",
    "\n",
    "\n",
    "def generate_text(image_path, task):\n",
    "    blip_layer = BlipCaptionSummaryLayer(processor, blip_model)\n",
    "    return blip_layer([image_path, task])\n",
    "\n",
    "\n",
    "# Replace with a valid image path from your dataset\n",
    "\n",
    "image_url = \"aircraft_damage_dataset_v1/test/dent/example.jpg\"\n",
    "\n",
    "img = plt.imread(image_url)\n",
    "plt.imshow(img)\n",
    "plt.axis('off') \n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Replace with a valid image path from your dataset\n",
    "\n",
    "image_path = tf.constant(\n",
    "    \"aircraft_damage_dataset_v1/test/dent/example.jpg\"\n",
    ")\n",
    "\n",
    "caption = generate_text(image_path, tf.constant(\"caption\"))\n",
    "summary = generate_text(image_path, tf.constant(\"summary\"))\n",
    "\n",
    "print(\"Caption:\", caption.numpy().decode(\"utf-8\"))\n",
    "print(\"Summary:\", summary.numpy().decode(\"utf-8\"))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e499205",
   "metadata": {},
   "source": [
    "### Results Summary\n",
    "\n",
    "- Binary damage classification achieved high accuracy using VGG16 feature extraction.\n",
    "- The BLIP model generated coherent captions and summaries describing aircraft damage.\n",
    "- The combined CV + NLP pipeline improves interpretability and inspection efficiency.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
